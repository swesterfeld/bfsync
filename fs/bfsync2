#!/usr/bin/env python

# bfsync: Big File synchronization based on Git

# Copyright (C) 2011 Stefan Westerfeld
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import subprocess
import hashlib
import pickle
import traceback
import time
import tempfile
import CfgParser
import HashCache
import StatusLine
import shutil
import argparse
import sqlite3

from utils import *
from TransferList import TransferList, TransferFile
from StatusLine import status_line
from HashCache import hash_cache
from ServerConn import ServerConn
from stat import *

def find_bfsync_dir():
  old_cwd = os.getcwd()
  dir = old_cwd
  while True:
    try:
      test_dir = os.path.join (dir, ".bfsync")
      os.chdir (test_dir)
      os.chdir (old_cwd)
      return test_dir
    except:
      pass
    # try parent directory
    newdir = os.path.dirname (dir)
    if newdir == dir:
      # no more parent
      raise Exception ("can not find .bfsync directory")
    dir = newdir

def commit_msg_ok (filename):
  file = open (filename, "r")
  result = False
  for line in file:
    line = line.strip()
    if len (line):
      if line[0] == "#":
        pass
      else:
        result = True
  file.close()
  return result

def make_object_filename (hash):
  if len (hash) != 40:
    raise Exception ("bad hash %s (not len 40)" % hash)
  return hash[0:2] + "/" + hash[2:]

def validate_object (object_file, hash):
  try:
    os.stat (object_file)
    if hash_cache.compute_hash (object_file) == hash:
      return True
  except:
    pass
  return False

def cmd_commit():
  parser = argparse.ArgumentParser (prog='bfsync2 commit')
  parser.add_argument ('-m', help='set commit message')
  commit_args = parser.parse_args (args)

  conn, repo_path = cd_repo_connect_db()

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  hash_list = []
  file_list = []

  c = conn.cursor()

  c.execute ('''SELECT id FROM inodes WHERE hash = "new"''')
  for row in c:
    id = row[0]
    filename = os.path.join (repo_path, "new", id[0:2], id[2:])
    hash_list += [ filename ]
    file_list += [ (filename, id) ]

  #hash_cache.hash_all (hash_list)
  #status_line.cleanup()

  # add new files via BFSync::Server
  add_new_list = []
  for (filename, id) in file_list:
    hash = hash_cache.compute_hash (filename)
    add_new_list += [id, hash]

  status_line.set_op ("ADD-NEW")
  files_added = 0
  files_total = len (add_new_list) / 2
  while len (add_new_list) > 0:
    items = min (len (add_new_list), 200)
    server_conn.add_new (add_new_list[0:items])
    add_new_list = add_new_list[items:]
    files_added += items / 2
    status_line.update ("file %d/%d" % (files_added, files_total))
  status_line.cleanup()

  server_conn.save_changes()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)

  # compute commit diff
  status_line.set_op ("COMMIT-DIFF")
  status_line.update ("computing changes between version %d and %d... " % (VERSION - 1, VERSION))
  diff_filename = os.path.join (repo_path, "tmp-commit-diff")
  if os.system ("bfdiff.py %d %d | xz -9 > %s" % (VERSION - 1, VERSION, diff_filename)) != 0:
    raise Exception ("error building commit diff")
  hash = hash_cache.compute_hash (diff_filename)
  diff_object_name = os.path.join (repo_path, "objects", make_object_filename (hash))
  if os.path.exists (diff_object_name):
    # already known
    os.unlink (diff_filename)
  else:
    # add new object
    os.rename (diff_filename, diff_object_name)
    os.chmod (diff_object_name, 0400)
  status_line.update ("done.")
  status_line.cleanup()

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE history SET message="commit message", author="author", hash=? WHERE version=?''', (hash, VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))
  conn.commit()
  c.close()

  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()
  return

class RemoteFile:
  pass

def cmd_remote_ls():
  file_list = []
  for rdir in args:
    os.chdir (rdir)
    object_dir = find_repo_dir() + "/objects"
    for dir, dirs, files in os.walk (object_dir):
      for f in files:
        full_name = os.path.join (dir, f)
        if os.path.isfile (full_name):
          name_hash = os.path.basename (dir) + f
          real_hash = hash_cache.compute_hash (full_name)
          if (name_hash == real_hash):
            remote_file = RemoteFile()
            remote_file.hash = real_hash
            remote_file.size = os.path.getsize (full_name)
            file_list += [ remote_file ]
  print pickle.dumps (file_list)

def cmd_remote_send():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.send_files (sys.stdout, False)

def cmd_debug_load_all_inodes():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["load-all-inodes"])[0]

def cmd_debug_perf_getattr():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["perf-getattr", args[0], args[1]])[0]

def cmd_debug_clear_cache():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()
  server_conn.clear_cache()

def cmd_debug_integrity():
  conn, repo_path = cd_repo_connect_db()

  c = conn.cursor()
  c.execute ('''SELECT vmin, vmax,id FROM inodes''')
  fail = False
  inode_d = dict()
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s" % (version, row[2])
      if inode_d.has_key (s):
        print "error: version %d available more than once for inode %s" % (version, row[2])
        fail = True
      inode_d[s] = 1
      version += 1

  c.execute ('''SELECT vmin, vmax, dir_id, inode_id FROM links''')
  link_d = dict()
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s|%s" % (version, row[2], row[3])
      if inode_d.has_key (s):
        print "error: version %d available more than once for link %s->%s" % (version, row[2], row[3])
        fail = True
      inode_d[s] = 1
      version += 1

  c.close()
  if fail:
    sys.exit (1)
  print "ok"
  return

def cmd_log():
  conn, repo_path = cd_repo_connect_db()

  c = conn.cursor()
  c.execute ('''SELECT * FROM history''')
  for row in c:
    version = row[0]
    hash    = row[1]
    author  = row[2]
    commit  = row[3]

    if commit != "":
      print "%4d   hash '%s', author '%s', msg '%s'" % (version, hash, author, commit)
  return

def cmd_remote_history():
  os.chdir (args[0])
  conn, repo_path = cd_repo_connect_db()
  c = conn.cursor()
  c.execute ('''SELECT * FROM history''')
  hlist = []
  for row in c:
    hlist += [ row ]
  print pickle.dumps (hlist)
  return

def cmd_pull():
  conn, repo_path = cd_repo_connect_db()

  url = args[0]
  (host, path) = url.split (":")
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-history", path], stdout=subprocess.PIPE).communicate()[0]
  remote_history = pickle.loads (remote_p)

  c = conn.cursor()
  c.execute ('''SELECT * FROM history''')

  local_history = []
  for row in c:
    local_history += [ row ]

  l_dict = dict()     # dict: version number -> diff hash
  for lh in local_history:
    version = lh[0]
    hash = lh[1]
    if hash:
      l_dict[version] = hash

  ff_transfer = []

  for rh in remote_history:
    version = rh[0]
    hash = rh[1]
    if hash:
      if l_dict.has_key (version):
        if hash != l_dict[version]:
          raise Exception ("incompatible version %d, would need merge algorithm to do this" % version)
        else:
          pass    # same version, local and remote
      else:
        ff_transfer += [ hash ]

  print "will fast-forward %d versions..." % len (ff_transfer)
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-ls", path], stdout=subprocess.PIPE).communicate()[0]
  remote_list = pickle.loads (remote_p)
  tlist = TransferList()
  for rfile in remote_list:
    if rfile.hash in ff_transfer:
      src_file = os.path.join (path, "objects", make_object_filename (rfile.hash))
      dest_file = os.path.join ("objects", make_object_filename (rfile.hash))
      if not validate_object (dest_file, rfile.hash):
        tlist.add (TransferFile (src_file, dest_file, rfile.size, 0400))
  remote_send_p = subprocess.Popen (["ssh", host, "bfsync2", "remote-send"],
                                    stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
  tlist.send_list (remote_send_p.stdin)
  tlist.receive_files (remote_send_p.stdout, True)
  for diff in ff_transfer:
    diff_file = os.path.join ("objects", make_object_filename (diff))
    print "applying patch %s" % diff_file
    os.system ("xzcat %s | bfapply.py" % diff_file)
  return

command = None
command_func = None
arg_iter = sys.argv[1:].__iter__()
args = []

for arg in arg_iter:
  commands = [
    ( "commit",         cmd_commit, 1),
    ( "log",            cmd_log, 0),
    ( "pull",           cmd_pull, 1),
    ( "remote-history", cmd_remote_history, 1),
    ( "remote-ls",      cmd_remote_ls, 1),
    ( "remote-send",    cmd_remote_send, 0),
    ( "debug-load-all-inodes",  cmd_debug_load_all_inodes, 0),
    ( "debug-perf-getattr",     cmd_debug_perf_getattr, 1),
    ( "debug-clear-cache",      cmd_debug_clear_cache, 1),
    ( "debug-integrity",        cmd_debug_integrity, 0),
  ]
  parse_ok = False
  if command == None:
    for c in commands:
      if c[0] == arg:
        command_func = c[1]
        command_args = c[2]
        command = c[0]
        parse_ok = True
  else:
    if command_args > 0:
      args += [ arg ]
      parse_ok = True
  if not parse_ok:
    sys.stderr.write ("can't parse command line args...\n")
    sys.exit (1)

if command_func != None:
  try:
    command_func()
  except Exception, ex:
    print "\n\n"
    print "=================================================="
    traceback.print_exc()
    print "=================================================="
    print "\n\n"
    hash_cache.save()
    sys.stderr.write ("bfsync2: %s\n" % ex)
    sys.exit (1)
  hash_cache.save()
else:
  print "usage: bfsync <command> [ args... ]"
