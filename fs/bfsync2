#!/usr/bin/env python

# bfsync: Big File synchronization based on Git

# Copyright (C) 2011 Stefan Westerfeld
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import subprocess
import hashlib
import pickle
import traceback
import time
import tempfile
import CfgParser
import HashCache
import StatusLine
import shutil
import argparse
import sqlite3

from utils import *
from TransferList import TransferList, TransferFile
from StatusLine import status_line
from HashCache import hash_cache
from ServerConn import ServerConn
from stat import *

def find_bfsync_dir():
  old_cwd = os.getcwd()
  dir = old_cwd
  while True:
    try:
      test_dir = os.path.join (dir, ".bfsync")
      os.chdir (test_dir)
      os.chdir (old_cwd)
      return test_dir
    except:
      pass
    # try parent directory
    newdir = os.path.dirname (dir)
    if newdir == dir:
      # no more parent
      raise Exception ("can not find .bfsync directory")
    dir = newdir

def commit_msg_ok (filename):
  file = open (filename, "r")
  result = False
  for line in file:
    line = line.strip()
    if len (line):
      if line[0] == "#":
        pass
      else:
        result = True
  file.close()
  return result

def cmd_commit():
  parser = argparse.ArgumentParser (prog='bfsync2 commit')
  parser.add_argument ('-m', help='set commit message')
  commit_args = parser.parse_args (args)

  conn, repo_path = cd_repo_connect_db()

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  hash_list = []
  file_list = []

  c = conn.cursor()

  c.execute ('''SELECT id FROM inodes WHERE hash = "new"''')
  for row in c:
    id = row[0]
    filename = os.path.join (repo_path, "new", id[0:2], id[2:])
    hash_list += [ filename ]
    file_list += [ (filename, id) ]

  #hash_cache.hash_all (hash_list)
  #status_line.cleanup()

  # add new files via BFSync::Server
  add_new_list = []
  for (filename, id) in file_list:
    hash = hash_cache.compute_hash (filename)
    add_new_list += [id, hash]

  status_line.set_op ("ADD-NEW")
  files_added = 0
  files_total = len (add_new_list) / 2
  while len (add_new_list) > 0:
    items = min (len (add_new_list), 200)
    server_conn.add_new (add_new_list[0:items])
    add_new_list = add_new_list[items:]
    files_added += items / 2
    status_line.update ("file %d/%d" % (files_added, files_total))
  status_line.cleanup()

  server_conn.save_changes()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)

  # compute commit diff
  status_line.set_op ("COMMIT-DIFF")
  status_line.update ("computing changes between version %d and %d... " % (VERSION - 1, VERSION))
  diff_filename = os.path.join (repo_path, "tmp-commit-diff")
  if os.system ("bfdiff.py %d %d | xz -9 > %s" % (VERSION - 1, VERSION, diff_filename)) != 0:
    raise Exception ("error building commit diff")
  hash = hash_cache.compute_hash (diff_filename)
  diff_object_name = os.path.join (repo_path, "objects", make_object_filename (hash))
  if os.path.exists (diff_object_name):
    # already known
    os.unlink (diff_filename)
  else:
    # add new object
    os.rename (diff_filename, diff_object_name)
    os.chmod (diff_object_name, 0400)
  status_line.update ("done.")
  status_line.cleanup()

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE history SET message="commit message", author="author", hash=? WHERE version=?''', (hash, VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))
  conn.commit()
  c.close()

  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()
  return

class RemoteFile:
  pass

def cmd_remote_ls():
  file_list = []
  for rdir in args:
    os.chdir (rdir)
    object_dir = find_repo_dir() + "/objects"
    for dir, dirs, files in os.walk (object_dir):
      for f in files:
        full_name = os.path.join (dir, f)
        if os.path.isfile (full_name):
          name_hash = os.path.basename (dir) + f
          real_hash = hash_cache.compute_hash (full_name)
          if (name_hash == real_hash):
            remote_file = RemoteFile()
            remote_file.hash = real_hash
            remote_file.size = os.path.getsize (full_name)
            file_list += [ remote_file ]
  print pickle.dumps (file_list)

def cmd_remote_send():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.send_files (sys.stdout, False)

def cmd_debug_load_all_inodes():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["load-all-inodes"])[0]

def cmd_debug_perf_getattr():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["perf-getattr", args[0], args[1]])[0]

def cmd_debug_clear_cache():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()
  server_conn.clear_cache()

def cmd_debug_integrity():
  conn, repo_path = cd_repo_connect_db()

  c = conn.cursor()
  c.execute ('''SELECT vmin, vmax,id FROM inodes''')
  fail = False
  inode_d = dict()
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s" % (version, row[2])
      if inode_d.has_key (s):
        print "error: version %d available more than once for inode %s" % (version, row[2])
        fail = True
      inode_d[s] = 1
      version += 1

  c.execute ('''SELECT vmin, vmax, dir_id, inode_id FROM links''')
  link_d = dict()
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s|%s" % (version, row[2], row[3])
      if inode_d.has_key (s):
        print "error: version %d available more than once for link %s->%s" % (version, row[2], row[3])
        fail = True
      inode_d[s] = 1
      version += 1

  c.close()
  if fail:
    sys.exit (1)
  print "ok"
  return

def cmd_log():
  conn, repo_path = cd_repo_connect_db()

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  for row in c:
    version = row[0]
    hash    = row[1]
    author  = row[2]
    commit  = row[3]

    if commit != "":
      print "%4d   hash '%s', author '%s', msg '%s'" % (version, hash, author, commit)
  return

def printable_name (c, id, VERSION):
  if id == "0" * 40:
    return "/"
  c.execute ("SELECT dir_id, name FROM links WHERE inode_id=? AND ? >= vmin AND ? <= VMAX", (id, VERSION, VERSION))
  for row in c:
    return os.path.join (printable_name (c, row[0], VERSION), row[1])
  return "*unknown*"

def cmd_status():
  conn, repo_path = cd_repo_connect_db()

  c = conn.cursor()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)
  c.execute ('''SELECT id FROM inodes WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  inode_ids = []
  for row in c:
    inode_ids += [ row[0] ]
  c.execute ('''SELECT dir_id, name, inode_id FROM links WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  link_ids = []
  for row in c:
    link_ids += [ row[0:3] ]
  print "%d inodes modified:" % len (inode_ids)
  print
  for id in inode_ids:
    print " - %s, name %s" % (id, printable_name (c, id, VERSION))
  print
  print "%d links modified:" % len (link_ids)
  print
  for lid in link_ids:
    print " - %s->%s, name %s" % (lid[0], lid[2], os.path.join (printable_name (c, lid[0], VERSION), lid[1]))

def cmd_revert():
  conn, repo_path = cd_repo_connect_db()
  c = conn.cursor()

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  if len (args) == 0:
    VERSION = 1
    c.execute ('''SELECT version FROM history''')
    for row in c:
      VERSION = max (row[0], VERSION)
    VERSION -= 1
  else:
    VERSION = int (args[0])
  print "reverting to version %d..." % VERSION
  c.execute ('''SELECT vmin, vmax, id FROM inodes WHERE vmax >= ?''', (VERSION, ))
  del_inode_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    id = row[2]
    if (vmin > VERSION):
      del_inode_list += [ (vmin, id) ]
  for vmin, id in del_inode_list:
    print "deleting %s/%d..." % (id, vmin)
    c.execute ('''DELETE FROM inodes WHERE vmin=? AND id=?''', (vmin, id))
  c.execute ('''SELECT vmin, vmax, dir_id, inode_id FROM links WHERE vmax >= ?''', (VERSION, ))
  del_link_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    dir_id = row[2]
    inode_id = row[3]
    if (vmin > VERSION):
      del_link_list += [ (vmin, dir_id, inode_id) ]
  for vmin, dir_id, inode_id in del_link_list:
    print "deleting %s->%s/%d..." % (dir_id, inode_id, vmin)
    c.execute ('''DELETE FROM links WHERE vmin=? AND dir_id=? AND inode_id=?''', (vmin, dir_id, inode_id))

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''DELETE FROM history WHERE version > ?''', (VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))

  conn.commit()
  c.close()
  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()

def cmd_db_fingerprint():
  conn, repo_path = cd_repo_connect_db()
  c = conn.cursor()

  # lock repo to ensure changes are written before we do something
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  c.execute ("SELECT * FROM inodes")
  inode_l = []
  for row in c:
    s = "i\0"
    for f in row:
      s += "%s\0" % f
    inode_l += [ s ]
  inode_l.sort()
  c.execute ("SELECT * FROM links")
  link_l = []
  for row in c:
    s = "l\0"
    for f in row:
      s += "%s\0" % f
    link_l += [ s ]
  link_l.sort()
  c.execute ("SELECT * FROM history")
  history_l = []
  for row in c:
    s = "h\0"
    for f in row:
      s += "%s\0" % f
    history_l += [ s ]
  history_l.sort()
  all_str = ""
  for r in link_l + inode_l + history_l:
    all_str += r + "\0"
  print hashlib.sha1 (all_str).hexdigest()

def cmd_remote_history():
  os.chdir (args[0])
  conn, repo_path = cd_repo_connect_db()
  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  hlist = []
  for row in c:
    hlist += [ row ]
  print pickle.dumps (hlist)
  return

def load_diff (hash):
  obj_name = os.path.join ("objects", make_object_filename (hash))
  diff = subprocess.Popen(["xzcat", obj_name], stdout=subprocess.PIPE).communicate()[0]
  return diff

def history_merge (local_history, remote_history):
  print "MERGE"
  print "local history:"

  lh_dict = dict()
  for lh in local_history:
    version = lh[0]
    hash = lh[1]
    print " * v%d : %s" % (version, hash)
    lh_dict[version] = hash

  print "remote history:"

  for rh in remote_history:
    version = rh[0]
    hash = rh[1]
    print " * v%d : %s" % (version, hash)
    if lh_dict.has_key (version) and lh_dict[version] == hash:
      print "   common"
    elif hash != "":
      d = load_diff (rh[1])
      for change in parse_diff (d):
        print "|".join (change)
      print

def cmd_pull():
  conn, repo_path = cd_repo_connect_db()

  url = args[0]
  (host, path) = url.split (":")
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-history", path], stdout=subprocess.PIPE).communicate()[0]
  remote_history = pickle.loads (remote_p)

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')

  local_history = []
  for row in c:
    local_history += [ row ]

  l_dict = dict()     # dict: version number -> diff hash
  for lh in local_history:
    version = lh[0]
    hash = lh[1]
    if hash:
      l_dict[version] = hash

  ff_apply = []
  transfer_objs = []
  can_fast_forward = True
  for rh in remote_history:
    version = rh[0]
    hash = rh[1]
    if hash:
      transfer_objs += [ hash ]
      if l_dict.has_key (version):
        if hash != l_dict[version]:
          can_fast_forward = False
        else:
          pass    # same version, local and remote
      else:
        ff_apply += [ hash ]

  # transfer required history objects
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-ls", path], stdout=subprocess.PIPE).communicate()[0]
  remote_list = pickle.loads (remote_p)
  tlist = TransferList()
  for rfile in remote_list:
    if rfile.hash in transfer_objs:
      src_file = os.path.join (path, "objects", make_object_filename (rfile.hash))
      dest_file = os.path.join ("objects", make_object_filename (rfile.hash))
      if not validate_object (dest_file, rfile.hash):
        tlist.add (TransferFile (src_file, dest_file, rfile.size, 0400))
  remote_send_p = subprocess.Popen (["ssh", host, "bfsync2", "remote-send"],
                                    stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
  tlist.send_list (remote_send_p.stdin)
  tlist.receive_files (remote_send_p.stdout, True)

  if can_fast_forward:
    print "will fast-forward %d versions..." % len (ff_apply)
    for diff in ff_apply:
      diff_file = os.path.join ("objects", make_object_filename (diff))
      print "applying patch %s" % diff
      os.system ("xzcat %s | bfapply.py" % diff_file)
  else:
    history_merge (local_history, remote_history)
  return

command = None
command_func = None
arg_iter = sys.argv[1:].__iter__()
args = []

for arg in arg_iter:
  commands = [
    ( "commit",         cmd_commit, 1),
    ( "log",            cmd_log, 0),
    ( "pull",           cmd_pull, 1),
    ( "status",         cmd_status, 0),
    ( "revert",         cmd_revert, 1),
    ( "db-fingerprint",  cmd_db_fingerprint, 0),
    ( "remote-history", cmd_remote_history, 1),
    ( "remote-ls",      cmd_remote_ls, 1),
    ( "remote-send",    cmd_remote_send, 0),
    ( "debug-load-all-inodes",  cmd_debug_load_all_inodes, 0),
    ( "debug-perf-getattr",     cmd_debug_perf_getattr, 1),
    ( "debug-clear-cache",      cmd_debug_clear_cache, 1),
    ( "debug-integrity",        cmd_debug_integrity, 0),
  ]
  parse_ok = False
  if command == None:
    for c in commands:
      if c[0] == arg:
        command_func = c[1]
        command_args = c[2]
        command = c[0]
        parse_ok = True
  else:
    if command_args > 0:
      args += [ arg ]
      parse_ok = True
  if not parse_ok:
    sys.stderr.write ("can't parse command line args...\n")
    sys.exit (1)

if command_func != None:
  try:
    command_func()
  except Exception, ex:
    print "\n\n"
    print "=================================================="
    traceback.print_exc()
    print "=================================================="
    print "\n\n"
    hash_cache.save()
    sys.stderr.write ("bfsync2: %s\n" % ex)
    sys.exit (1)
  hash_cache.save()
else:
  print "usage: bfsync <command> [ args... ]"
