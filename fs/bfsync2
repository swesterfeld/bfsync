#!/usr/bin/env python

# bfsync: Big File synchronization based on Git

# Copyright (C) 2011 Stefan Westerfeld
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import subprocess
import hashlib
import pickle
import traceback
import time
import tempfile
import CfgParser
import HashCache
import StatusLine
import shutil
import argparse
import sqlite3

from utils import *
from dbutils import *
from TransferList import TransferList, TransferFile
from StatusLine import status_line
from HashCache import hash_cache
from ServerConn import ServerConn
from stat import *

def find_bfsync_dir():
  old_cwd = os.getcwd()
  dir = old_cwd
  while True:
    try:
      test_dir = os.path.join (dir, ".bfsync")
      os.chdir (test_dir)
      os.chdir (old_cwd)
      return test_dir
    except:
      pass
    # try parent directory
    newdir = os.path.dirname (dir)
    if newdir == dir:
      # no more parent
      raise Exception ("can not find .bfsync directory")
    dir = newdir

def commit_msg_ok (filename):
  file = open (filename, "r")
  result = False
  for line in file:
    line = line.strip()
    if len (line):
      if line[0] == "#":
        pass
      else:
        result = True
  file.close()
  return result

def cmd_commit():
  parser = argparse.ArgumentParser (prog='bfsync2 commit')
  parser.add_argument ('-m', help='set commit message')
  commit_args = parser.parse_args (args)

  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  hash_list = []
  file_list = []

  c = conn.cursor()

  c.execute ('''SELECT id FROM inodes WHERE hash = "new"''')
  for row in c:
    id = row[0]
    filename = os.path.join (repo_path, "new", id[0:2], id[2:])
    hash_list += [ filename ]
    file_list += [ (filename, id) ]

  #hash_cache.hash_all (hash_list)
  #status_line.cleanup()

  # add new files via BFSync::Server
  add_new_list = []
  for (filename, id) in file_list:
    hash = hash_cache.compute_hash (filename)
    add_new_list += [id, hash]

  status_line.set_op ("ADD-NEW")
  files_added = 0
  files_total = len (add_new_list) / 2
  while len (add_new_list) > 0:
    items = min (len (add_new_list), 200)
    server_conn.add_new (add_new_list[0:items])
    add_new_list = add_new_list[items:]
    files_added += items / 2
    status_line.update ("file %d/%d" % (files_added, files_total))
  status_line.cleanup()

  server_conn.save_changes()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)

  # compute commit diff
  status_line.set_op ("COMMIT-DIFF")
  status_line.update ("computing changes between version %d and %d... " % (VERSION - 1, VERSION))
  diff_filename = os.path.join (repo_path, "tmp-commit-diff")
  if os.system ("bfdiff.py %d %d | xz -9 > %s" % (VERSION - 1, VERSION, diff_filename)) != 0:
    raise Exception ("error building commit diff")
  hash = hash_cache.compute_hash (diff_filename)
  diff_object_name = os.path.join (repo_path, "objects", make_object_filename (hash))
  if os.path.exists (diff_object_name):
    # already known
    os.unlink (diff_filename)
  else:
    # add new object
    os.rename (diff_filename, diff_object_name)
    os.chmod (diff_object_name, 0400)
  status_line.update ("done.")
  status_line.cleanup()

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax = ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE history SET message="commit message", author="author", hash=? WHERE version=?''', (hash, VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))
  conn.commit()
  c.close()

  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()
  return

class RemoteFile:
  pass

def cmd_remote_ls():
  file_list = []
  for rdir in args:
    os.chdir (rdir)
    object_dir = find_repo_dir() + "/objects"
    for dir, dirs, files in os.walk (object_dir):
      for f in files:
        full_name = os.path.join (dir, f)
        if os.path.isfile (full_name):
          name_hash = os.path.basename (dir) + f
          real_hash = hash_cache.compute_hash (full_name)
          if (name_hash == real_hash):
            remote_file = RemoteFile()
            remote_file.hash = real_hash
            remote_file.size = os.path.getsize (full_name)
            file_list += [ remote_file ]
  print pickle.dumps (file_list)

def cmd_remote_send():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.send_files (sys.stdout, False)

def cmd_remote_receive():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.receive_files (sys.stdin, False)

def cmd_debug_load_all_inodes():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["load-all-inodes"])[0]

def cmd_debug_perf_getattr():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["perf-getattr", args[0], args[1]])[0]

def cmd_debug_clear_cache():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()
  server_conn.clear_cache()

def cmd_debug_integrity():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()
  c.execute ('''SELECT vmin, vmax,id FROM inodes''')
  fail = False
  inode_d = dict()
  conflicts = []
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s" % (version, row[2])
      if inode_d.has_key (s):
        conflicts += [ (version, row[2]) ]
      inode_d[s] = 1
      version += 1

  for conflict in conflicts:
    version = conflict[0]
    id = conflict[1]
    print "error: version %d available more than once for inode %s, name %s" % (
           version, id, printable_name (c, id, version))
    fail = True

  c.execute ('''SELECT vmin, vmax, dir_id, name FROM links''')
  link_d = dict()
  conflicts = []
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s|%s" % (version, row[2], row[3])
      if link_d.has_key (s):
        conflicts += [ (version, row[2], row[3]) ]
      link_d[s] = 1
      version += 1

  for conflict in conflicts:
    version = conflict[0]
    id = conflict[1]
    name = conflict[2]
    print "error: version %d available more than once for link %s->%s, name %s" % (
          version, id, name, os.path.join (printable_name (c, id, version), name))
    fail = True

  c.close()
  if fail:
    sys.exit (1)
  print "ok"
  return

def cmd_log():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  for row in c:
    version = row[0]
    hash    = row[1]
    author  = row[2]
    commit  = row[3]

    if commit != "":
      print "%4d   hash '%s', author '%s', msg '%s'" % (version, hash, author, commit)
  return

def printable_name (c, id, VERSION):
  if id == "0" * 40:
    return "/"
  c.execute ("SELECT dir_id, name FROM links WHERE inode_id=? AND ? >= vmin AND ? <= VMAX", (id, VERSION, VERSION))
  for row in c:
    return os.path.join (printable_name (c, row[0], VERSION), row[1])
  return "*unknown*"

def cmd_status():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)
  c.execute ('''SELECT id FROM inodes WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  inode_ids = []
  for row in c:
    inode_ids += [ row[0] ]
  c.execute ('''SELECT dir_id, name, inode_id FROM links WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  link_ids = []
  for row in c:
    link_ids += [ row[0:3] ]
  print "%d inodes modified:" % len (inode_ids)
  print
  for id in inode_ids:
    print " - %s, name %s" % (id, printable_name (c, id, VERSION))
  print
  print "%d links modified:" % len (link_ids)
  print
  for lid in link_ids:
    print " - %s->%s, name %s" % (lid[0], lid[2], os.path.join (printable_name (c, lid[0], VERSION), lid[1]))

def cmd_revert():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  if len (args) == 0:
    VERSION = 1
    c.execute ('''SELECT version FROM history''')
    for row in c:
      VERSION = max (row[0], VERSION)
    VERSION -= 1
  else:
    VERSION = int (args[0])
  print "reverting to version %d..." % VERSION
  c.execute ('''SELECT vmin, vmax, id FROM inodes WHERE vmax >= ?''', (VERSION, ))
  del_inode_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    id = row[2]
    if (vmin > VERSION):
      del_inode_list += [ (vmin, id) ]
  for vmin, id in del_inode_list:
    print "deleting %s/%d..." % (id, vmin)
    c.execute ('''DELETE FROM inodes WHERE vmin=? AND id=?''', (vmin, id))
  c.execute ('''SELECT vmin, vmax, dir_id, inode_id FROM links WHERE vmax >= ?''', (VERSION, ))
  del_link_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    dir_id = row[2]
    inode_id = row[3]
    if (vmin > VERSION):
      del_link_list += [ (vmin, dir_id, inode_id) ]
  for vmin, dir_id, inode_id in del_link_list:
    print "deleting %s->%s/%d..." % (dir_id, inode_id, vmin)
    c.execute ('''DELETE FROM links WHERE vmin=? AND dir_id=? AND inode_id=?''', (vmin, dir_id, inode_id))

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''DELETE FROM history WHERE version > ?''', (VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))

  conn.commit()
  c.close()
  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()

def cmd_db_fingerprint():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()

  # lock repo to ensure changes are written before we do something
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  c.execute ("SELECT * FROM inodes")
  inode_l = []
  for row in c:
    s = "i\0"
    for f in row:
      s += "%s\0" % f
    inode_l += [ s ]
  inode_l.sort()
  c.execute ("SELECT * FROM links")
  link_l = []
  for row in c:
    s = "l\0"
    for f in row:
      s += "%s\0" % f
    link_l += [ s ]
  link_l.sort()
  c.execute ("SELECT * FROM history")
  history_l = []
  for row in c:
    s = "h\0"
    for f in row:
      s += "%s\0" % f
    history_l += [ s ]
  history_l.sort()
  all_str = ""
  for r in link_l + inode_l + history_l:
    all_str += r + "\0"
  print hashlib.sha1 (all_str).hexdigest()

def cmd_remote_history():
  os.chdir (args[0])
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  hlist = []
  for row in c:
    hlist += [ row ]
  print pickle.dumps (hlist)
  return

def cmd_remote_history_update():
  os.chdir (args[0])
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()
  delta_history = pickle.loads (sys.stdin.read())
  fail = False
  for dh in delta_history:
    version = dh[0]
    for row in c.execute ("""SELECT * FROM history WHERE version=?""", (version,)):
      fail = True
    c.execute ("""INSERT INTO history VALUES (?,?,?,?,?)""", dh)
  if fail:
    print "fail"
    conn.rollback()
  else:
    print "ok"
    conn.commit()
  return


def load_diff (hash):
  obj_name = os.path.join ("objects", make_object_filename (hash))
  diff = subprocess.Popen(["xzcat", obj_name], stdout=subprocess.PIPE).communicate()[0]
  return diff

def db_contains_link (c, VERSION, dir_id, name):
  c.execute ("SELECT * FROM links WHERE dir_id = ? AND name = ? AND ? >= vmin AND ? <= VMAX",
             (dir_id, name, VERSION, VERSION))
  for row in c:
    return True
  return False

def history_merge (c, repo_path, local_history, remote_history):
  print "MERGE"

  common_version = 0
  for v in range (min (len (local_history), len (remote_history))):
    lh = local_history[v]
    rh = remote_history[v]
    # check version
    assert (lh[0] == v + 1)
    assert (rh[0] == v + 1)
    print "  ", lh[0], lh[1], rh[1]
    if lh[1] == rh[1]:
      common_version = v + 1
    else:
      break

  print "last common version:", common_version
  os.system ("bfsync2 revert %d" % common_version)
  print "apply patches:"

  for rh in remote_history:
    if rh[0] > common_version:
      diff = rh[1]
      diff_file = os.path.join ("objects", make_object_filename (diff))

      changes = parse_diff (load_diff (diff))
      for change in changes:
        print " => ", "|".join (change)

      print "applying patch %s" % diff
      os.system ("xzcat %s | bfapply.py" % diff_file)
  for lh in local_history:
    if lh[0] > common_version:

      # determine current db version
      VERSION = 1
      c.execute ('''SELECT version FROM history''')
      for row in c:
        VERSION = max (row[0], VERSION)

      # adapt diff to get rid of conflicts
      diff = lh[1]
      diff_file = os.path.join ("objects", make_object_filename (diff))

      new_diff_filename = os.path.join (repo_path, "tmp-merge-diff")
      new_diff_file = open (new_diff_filename, "w")

      changes = parse_diff (load_diff (diff))
      for change in changes:
        if change[0] == "l+":
          if db_contains_link (c, VERSION, change[1], change[2]):
            print "LINK CONFLICT"
            suffix = 1
            while db_contains_link (c, VERSION, change[1], change[2] + "~%d" % suffix):
              suffix += 1
            change[2] = change[2] + "~%d" % suffix
        print " => ", "|".join (change)
        s = ""
        for change_field in change:
          s += change_field + "\0"
        new_diff_file.write (s)

      new_diff_file.close()

      # apply modified diff
      print "applying patch %s" % diff
      os.system ("bfapply.py < %s" % new_diff_filename)

def cmd_pull():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  if len (args) == 0:
    default_pull = repo.config.get ("default/pull")
    if len (default_pull) == 0:
      raise Exception ("pull: no repository specified and default/push config value empty")
    url = default_pull[0]
  else:
    url = args[0]

  (host, path) = url.split (":")
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-history", path], stdout=subprocess.PIPE).communicate()[0]
  remote_history = pickle.loads (remote_p)

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')

  local_history = []
  for row in c:
    local_history += [ row ]

  l_dict = dict()     # dict: version number -> diff hash
  for lh in local_history:
    version = lh[0]
    hash = lh[1]
    if hash:
      l_dict[version] = hash

  ff_apply = []
  transfer_objs = []
  can_fast_forward = True
  for rh in remote_history:
    version = rh[0]
    hash = rh[1]
    if hash:
      transfer_objs += [ hash ]
      if l_dict.has_key (version):
        if hash != l_dict[version]:
          can_fast_forward = False
        else:
          pass    # same version, local and remote
      else:
        ff_apply += [ hash ]

  # transfer required history objects
  get_remote_objects (url, transfer_objs)

  if can_fast_forward:
    print "will fast-forward %d versions..." % len (ff_apply)
    for diff in ff_apply:
      diff_file = os.path.join ("objects", make_object_filename (diff))
      print "applying patch %s" % diff
      os.system ("xzcat %s | bfapply.py" % diff_file)
  else:
    history_merge (c, repo_path, local_history, remote_history)
  return

def cmd_push():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  if len (args) == 0:
    default_push = repo.config.get ("default/push")
    if len (default_push) == 0:
      raise Exception ("push: no repository specified and default/push config value empty")
    url = default_push[0]
  else:
    url = args[0]

  (host, path) = url.split (":")
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-history", path], stdout=subprocess.PIPE).communicate()[0]
  remote_history = pickle.loads (remote_p)

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')

  local_history = []
  for row in c:
    local_history += [ row ]

  common_version = 0
  for v in range (min (len (local_history), len (remote_history))):
    lh = local_history[v]
    rh = remote_history[v]
    # check version
    assert (lh[0] == v + 1)
    assert (rh[0] == v + 1)
    print "  ", lh[0], lh[1], rh[1]
    if lh[1] == rh[1]:
      common_version = v + 1
    else:
      break
  if common_version != len (remote_history):
    raise Exception ("push failed, remote history contains commits not in local history (pull to fix this)")

  print "PUSH:"
  delta_history = []
  for v in range (len (local_history)):
    if v + 1 > common_version:
      delta_history += [ local_history[v] ]
  remote_send_p = subprocess.Popen (["ssh", host, "bfsync2", "remote-history-update", path],
                                    stdin=subprocess.PIPE, stdout=subprocess.PIPE)
  remote_send_p.stdin.write (pickle.dumps (delta_history))
  print remote_send_p.communicate()[0]

  need_objs_p = subprocess.Popen (["ssh", host, "bfsync2", "remote-need-objects", path],
                                  stdout = subprocess.PIPE).communicate()[0]
  need_objs = pickle.loads (need_objs_p)

  tl = TransferList()
  for hash in need_objs:
    src_file = os.path.join ("objects", make_object_filename (hash))
    if validate_object (src_file, hash):
      tl.add (TransferFile (src_file, os.path.join (path, src_file), os.path.getsize (src_file), 0400))

  pipe = subprocess.Popen (["ssh", host, "bfsync2", "remote-receive"], bufsize=-1, stdin=subprocess.PIPE).stdin
  tl.send_list (pipe)
  tl.send_files (pipe, True)
  return

def get_remote_objects (url, transfer_objs):
  (host, path) = url.split (":")

  # make a list of hashes that we need
  print "checking which objects are needed..."
  need_hash = dict()
  for thash in transfer_objs:
    dest_file = os.path.join ("objects", make_object_filename (thash))
    if not validate_object (dest_file, thash):
      need_hash[thash] = True

  # check for objects in remote repo
  remote_p = subprocess.Popen(["ssh", host, "bfsync2", "remote-ls", path], stdout=subprocess.PIPE).communicate()[0]
  remote_list = pickle.loads (remote_p)
  tlist = TransferList()
  print "generating transfer list..."
  for rfile in remote_list:
    if need_hash.has_key (rfile.hash):
      src_file = os.path.join (path, "objects", make_object_filename (rfile.hash))
      dest_file = os.path.join ("objects", make_object_filename (rfile.hash))
      tlist.add (TransferFile (src_file, dest_file, rfile.size, 0400))

  # do the actual copying
  print "transferring objects..."
  remote_send_p = subprocess.Popen (["ssh", host, "bfsync2", "remote-send"],
                                    stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
  tlist.send_list (remote_send_p.stdin)
  tlist.receive_files (remote_send_p.stdout, True)

def cmd_get():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  url = args[0]

  c = conn.cursor()

  print "create object list..."
  # create list of required objects
  objs = []
  c.execute ('''SELECT DISTINCT hash FROM inodes''')
  for row in c:
    s = "%s" % row[0]
    if len (s) == 40:
      objs += [ s ]

  get_remote_objects (url, objs)

def cmd_remote_need_objects():
  os.chdir (args[0])

  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()

  # MERGE ME WITH get
  c.execute ('''SELECT DISTINCT hash FROM history''')
  objs = []
  for row in c:
    hash = "%s" % row[0]
    if len (hash) == 40:
      dest_file = os.path.join ("objects", make_object_filename (hash))
      if not validate_object (dest_file, hash):
        objs += [ hash ]
  # end MERGE ME
  print pickle.dumps (objs)


def cmd_init():
  dir = args[0]
  try:
    os.mkdir (dir, 0700)
  except:
    raise Exception ("can't create directory %s for repository" % dir)
  bfsync_dir = os.path.join (dir, ".bfsync")
  os.mkdir (bfsync_dir)
  f = open (os.path.join (bfsync_dir, "info"), "w")
  f.write ("repo-type master;")
  f.close()
  f = open (os.path.join (bfsync_dir, "config"), "w")
  f.write ("sqlite-sync 1;")
  f.close()
  db_file = os.path.join (dir, "db")
  conn = sqlite3.connect (db_file)
  c = conn.cursor()
  c.execute ('''CREATE TABLE history
                 (
                   version integer,
                   hash    text,
                   author  text,
                   message text,
                   time    integer
                 )''')
  conn.commit()

def guess_dir_name (url):
  dir_name = ""
  for ch in reversed (url):
    if (ch == ":") or (ch == "/"):
      return dir_name
    dir_name = ch + dir_name
  return url

def cmd_clone():
  url = args[0]
  dir = guess_dir_name (args[0])
  if os.path.exists (dir):
    print "fatal: destination path '" + dir + "' already exists"
    sys.exit (1)

  print url, "=>", dir

  try:
    os.mkdir (dir, 0700)
  except:
    raise Exception ("can't create directory %s for repository" % dir)

  bfsync_dir = os.path.join (dir, ".bfsync")
  os.mkdir (bfsync_dir)

  # init .bfsync/info
  f = open (os.path.join (bfsync_dir, "info"), "w")
  f.write ("repo-type store;")
  f.close()

  # default config
  f = open (os.path.join (bfsync_dir, "config"), "w")
  f.write ("sqlite-sync 1;\n")
  f.write ("default {\n")
  f.write ("""  pull "%s";\n""" % url)
  f.write ("""  push "%s";\n""" % url)
  f.write ("}\n")
  f.close()

  os.chdir (dir)
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()
  create_tables (c)
  init_tables (c)
  conn.commit()

  os.mkdir ("new", 0700)
  os.mkdir ("objects", 0700)
  for i in range (0, 256):
    os.mkdir ("new/%02x" % i, 0700)
    os.mkdir ("objects/%02x" % i, 0700)

  # mount for pull
  os.mkdir ("clone-mnt", 0700)
  if subprocess.call (["bfsyncfs", ".", "clone-mnt"]) != 0:
    raise Exception ("cannot mount repo")
  os.chdir ("clone-mnt")
  if subprocess.call (["bfsync2", "pull", url]) != 0:
    raise Exception ("cannot pull from repo")
  os.chdir ("..")
  if subprocess.call (["fusermount", "-u", "clone-mnt"]) != 0:
    raise Exception ("cannot umount repo")
  os.rmdir ("clone-mnt")
  # FIXME: ensure repo deletion on exception

command = None
command_func = None
arg_iter = sys.argv[1:].__iter__()
args = []

for arg in arg_iter:
  commands = [
    ( "commit",                 cmd_commit, 1),
    ( "log",                    cmd_log, 0),
    ( "pull",                   cmd_pull, 1),
    ( "push",                   cmd_push, 1),
    ( "get",                    cmd_get, 1),
    ( "status",                 cmd_status, 0),
    ( "revert",                 cmd_revert, 1),
    ( "init",                   cmd_init, 1),
    ( "clone",                  cmd_clone, 1),
    ( "db-fingerprint",         cmd_db_fingerprint, 0),
    ( "remote-history",         cmd_remote_history, 1),
    ( "remote-history-update",  cmd_remote_history_update, 1),
    ( "remote-ls",              cmd_remote_ls, 1),
    ( "remote-need-objects",    cmd_remote_need_objects, 1),
    ( "remote-receive",         cmd_remote_receive, 0),
    ( "remote-send",            cmd_remote_send, 0),
    ( "debug-load-all-inodes",  cmd_debug_load_all_inodes, 0),
    ( "debug-perf-getattr",     cmd_debug_perf_getattr, 1),
    ( "debug-clear-cache",      cmd_debug_clear_cache, 1),
    ( "debug-integrity",        cmd_debug_integrity, 0),
  ]
  parse_ok = False
  if command == None:
    for c in commands:
      if c[0] == arg:
        command_func = c[1]
        command_args = c[2]
        command = c[0]
        parse_ok = True
  else:
    if command_args > 0:
      args += [ arg ]
      parse_ok = True
  if not parse_ok:
    sys.stderr.write ("can't parse command line args...\n")
    sys.exit (1)

if command_func != None:
  try:
    command_func()
  except Exception, ex:
    print "\n\n"
    print "=================================================="
    traceback.print_exc()
    print "=================================================="
    print "\n\n"
    hash_cache.save()
    sys.stderr.write ("bfsync2: %s\n" % ex)
    sys.exit (1)
  hash_cache.save()
else:
  print "usage: bfsync <command> [ args... ]"
