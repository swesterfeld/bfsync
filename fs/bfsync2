#!/usr/bin/env python

# bfsync: Big File synchronization based on Git

# Copyright (C) 2011 Stefan Westerfeld
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import subprocess
import hashlib
import cPickle
import traceback
import time
import tempfile
import CfgParser
import HashCache
import StatusLine
import shutil
import argparse
import sqlite3
import shutil
import datetime

from utils import *
from dbutils import *
from diffutils import diff
from applyutils import apply
from commitutils import commit
from TransferList import TransferList, TransferFile
from StatusLine import status_line
from HashCache import hash_cache
from ServerConn import ServerConn
from RemoteRepo import RemoteRepo
from stat import *

def find_bfsync_dir():
  old_cwd = os.getcwd()
  dir = old_cwd
  while True:
    try:
      test_dir = os.path.join (dir, ".bfsync")
      os.chdir (test_dir)
      os.chdir (old_cwd)
      return test_dir
    except:
      pass
    # try parent directory
    newdir = os.path.dirname (dir)
    if newdir == dir:
      # no more parent
      raise Exception ("can not find .bfsync directory")
    dir = newdir

def commit_msg_ok (filename):
  file = open (filename, "r")
  result = False
  for line in file:
    line = line.strip()
    if len (line):
      if line[0] == "#":
        pass
      else:
        result = True
  file.close()
  return result

def cmd_commit():
  parser = argparse.ArgumentParser (prog='bfsync2 commit')
  parser.add_argument ('-m', help='set commit message')
  commit_args = parser.parse_args (args)

  repo = cd_repo_connect_db()
  commit (repo)

class RemoteFile:
  pass

def cmd_debug_load_all_inodes():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["load-all-inodes"])[0]

def cmd_debug_perf_getattr():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  print server_conn.process_call (["perf-getattr", args[0], args[1]])[0]

def cmd_debug_clear_cache():
  bfsync_dir = find_bfsync_dir()

  bfsync_info = parse_config (bfsync_dir + "/info")

  repo_path = bfsync_info.get ("repo-path")
  if len (repo_path) != 1:
    raise Exception ("bad repo path")
  repo_path = repo_path[0]

  os.chdir (repo_path)
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()
  server_conn.clear_cache()

def cmd_debug_integrity():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()
  c.execute ('''SELECT vmin, vmax,id FROM inodes''')
  fail = False
  inode_d = dict()
  conflicts = []
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s" % (version, row[2])
      if inode_d.has_key (s):
        conflicts += [ (version, row[2]) ]
      inode_d[s] = 1
      version += 1

  for conflict in conflicts:
    version = conflict[0]
    id = conflict[1]
    print "error: version %d available more than once for inode %s, name %s" % (
           version, id, printable_name (c, id, version))
    fail = True

  c.execute ('''SELECT vmin, vmax, dir_id, name FROM links''')
  link_d = dict()
  conflicts = []
  for row in c:
    vmin = int (row[0])
    vmax = int (row[1])
    version = vmin
    while version <= vmax:
      s = "%d|%s|%s" % (version, row[2], row[3])
      if link_d.has_key (s):
        conflicts += [ (version, row[2], row[3]) ]
      link_d[s] = 1
      version += 1

  for conflict in conflicts:
    version = conflict[0]
    id = conflict[1]
    name = conflict[2]
    print "error: version %d available more than once for link %s->%s, name %s" % (
          version, id, name, os.path.join (printable_name (c, id, version), name))
    fail = True

  c.close()
  if fail:
    sys.exit (1)
  print "ok"
  return

def cmd_log():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  for row in c:
    version = row[0]
    hash    = row[1]
    author  = row[2]
    commit  = row[3]

    if commit != "":
      print "%4d   hash '%s', author '%s', msg '%s'" % (version, hash, author, commit)
  return

def printable_name (c, id, VERSION):
  if id == "0" * 40:
    return "/"
  c.execute ("SELECT dir_id, name FROM links WHERE inode_id=? AND ? >= vmin AND ? <= VMAX", (id, VERSION, VERSION))
  for row in c:
    return os.path.join (printable_name (c, row[0], VERSION), row[1])
  return "*unknown*"

def cmd_status():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()

  VERSION = 1
  c.execute ('''SELECT version FROM history''')
  for row in c:
    VERSION = max (row[0], VERSION)
  c.execute ('''SELECT id FROM inodes WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  inode_ids = []
  for row in c:
    inode_ids += [ row[0] ]
  c.execute ('''SELECT dir_id, name, inode_id FROM links WHERE vmin=%d AND vmax=%d''' % (VERSION, VERSION))
  link_ids = []
  for row in c:
    link_ids += [ row[0:3] ]
  print "%d inodes modified:" % len (inode_ids)
  print
  for id in inode_ids:
    print " - %s, name %s" % (id, printable_name (c, id, VERSION))
  print
  print "%d links modified:" % len (link_ids)
  print
  for lid in link_ids:
    print " - %s->%s, name %s" % (lid[0], lid[2], os.path.join (printable_name (c, lid[0], VERSION), lid[1]))

def cmd_revert():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  c = conn.cursor()

  # lock repo to allow modifications
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  if len (args) == 0:
    VERSION = 1
    c.execute ('''SELECT version FROM history''')
    for row in c:
      VERSION = max (row[0], VERSION)
    VERSION -= 1
  else:
    VERSION = int (args[0])
  print "reverting to version %d..." % VERSION
  c.execute ('''SELECT vmin, vmax, id FROM inodes WHERE vmax >= ?''', (VERSION, ))
  del_inode_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    id = row[2]
    if (vmin > VERSION):
      del_inode_list += [ (vmin, id) ]
  for vmin, id in del_inode_list:
    c.execute ('''DELETE FROM inodes WHERE vmin=? AND id=?''', (vmin, id))
  c.execute ('''SELECT vmin, vmax, dir_id, inode_id FROM links WHERE vmax >= ?''', (VERSION, ))
  del_link_list = []
  for row in c:
    vmin = row[0]
    vmax = row[1]
    dir_id = row[2]
    inode_id = row[3]
    if (vmin > VERSION):
      del_link_list += [ (vmin, dir_id, inode_id) ]
  for vmin, dir_id, inode_id in del_link_list:
    c.execute ('''DELETE FROM links WHERE vmin=? AND dir_id=? AND inode_id=?''', (vmin, dir_id, inode_id))

  c.execute ('''UPDATE inodes SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''UPDATE links SET vmax=? WHERE vmax >= ?''', (VERSION + 1, VERSION))
  c.execute ('''DELETE FROM history WHERE version > ?''', (VERSION, ))
  c.execute ('''INSERT INTO history VALUES (?,?,?,?,?)''', (VERSION + 1, "", "", "", 0))

  conn.commit()
  c.close()
  # we modified the db, so the fs needs to reload everything
  # in-memory cached items will not be correct
  server_conn.clear_cache()

def cmd_db_fingerprint():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()

  # lock repo to ensure changes are written before we do something
  server_conn = ServerConn (repo_path)
  server_conn.get_lock()

  c.execute ("SELECT * FROM inodes")
  inode_l = []
  for row in c:
    s = "i\0"
    for f in row:
      s += "%s\0" % f
    inode_l += [ s ]
  inode_l.sort()
  c.execute ("SELECT * FROM links")
  link_l = []
  for row in c:
    s = "l\0"
    for f in row:
      s += "%s\0" % f
    link_l += [ s ]
  link_l.sort()
  c.execute ("SELECT * FROM history")
  history_l = []
  for row in c:
    s = "h\0"
    for f in row:
      s += "%s\0" % f
    history_l += [ s ]
  history_l.sort()
  all_str = ""
  for r in link_l + inode_l + history_l:
    all_str += r + "\0"
  print hashlib.sha1 (all_str).hexdigest()

def remote_ls (repo):
  file_list = []
  object_dir = os.path.join (repo.path, "objects")
  for dir, dirs, files in os.walk (object_dir):
    for f in files:
      full_name = os.path.join (dir, f)
      if os.path.isfile (full_name):
        name_hash = os.path.basename (dir) + f
        real_hash = hash_cache.compute_hash (full_name)
        if (name_hash == real_hash):
          remote_file = RemoteFile()
          remote_file.hash = real_hash
          remote_file.size = os.path.getsize (full_name)
          file_list += [ remote_file ]
  return file_list

def remote_send():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.send_files (sys.stdout, False)
  sys.stdout.flush()

def remote_receive():
  tl = TransferList()
  tl.receive_list (sys.stdin)
  tl.receive_files (sys.stdin, False)

def remote_update_history (repo):
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()
  dhlen = int (sys.stdin.readline())
  delta_history = cPickle.loads (sys.stdin.read (dhlen))
  fail = False
  for dh in delta_history:
    version = dh[0]
    for row in c.execute ("""SELECT * FROM history WHERE version=?""", (version,)):
      fail = True
    c.execute ("""INSERT INTO history VALUES (?,?,?,?,?)""", dh)
  if fail:
    conn.rollback()
    return "fail"
  else:
    conn.commit()
    return "ok"

def remote_history (repo):
  conn = repo.conn
  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')
  hlist = []
  for row in c:
    hlist += [ row ]
  return cPickle.dumps (hlist)

def remote_need_objects (repo):
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()

  # MERGE ME WITH get
  c.execute ('''SELECT DISTINCT hash FROM history''')
  objs = []
  for row in c:
    hash = "%s" % row[0]
    if len (hash) == 40:
      dest_file = os.path.join (repo_path, "objects", make_object_filename (hash))
      if not validate_object (dest_file, hash):
        objs += [ hash ]
  # end MERGE ME
  return cPickle.dumps (objs)

def cmd_remote():
  os.chdir (args[0])
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()

  while True:
    command = sys.stdin.readline().strip()
    result = None
    if command == "history":
      result = remote_history (repo)
    elif command == "ls":
      result = cPickle.dumps (remote_ls (repo))
    elif command == "send":
      remote_send()
    elif command == "receive":
      remote_receive()
    elif command == "update-history":
      result = remote_update_history (repo)
    elif command == "need-objects":
      result = remote_need_objects (repo)
    elif command == "":
      return
    else:
      raise Exception ("unknown command in cmd_remote(): '%s'" % command)
    if not result is None:
      sys.stdout.write ("%s\n%s" % (len (result), result))
      sys.stdout.flush()

def load_diff (hash):
  obj_name = os.path.join ("objects", make_object_filename (hash))
  diff = subprocess.Popen(["xzcat", obj_name], stdout=subprocess.PIPE).communicate()[0]
  return diff

def db_contains_link (c, VERSION, dir_id, name):
  c.execute ("SELECT * FROM links WHERE dir_id = ? AND name = ? AND ? >= vmin AND ? <= vmax",
             (dir_id, name, VERSION, VERSION))
  for row in c:
    return True
  return False

def db_link_inode (c, VERSION, dir_id, name):
  c.execute ("SELECT inode_id FROM links WHERE dir_id = ? AND name = ? AND ? >= vmin AND ? <= vmax",
             (dir_id, name, VERSION, VERSION))
  for row in c:
    return row[0]
  raise Exception ("link target for %s/%s not found" % (dir_id, name))

def db_inode (c, VERSION, id):
  c.execute ("SELECT * FROM inodes WHERE id = ? AND ? >= vmin AND ? <= vmax",
             (id, VERSION, VERSION))
  for row in c:
    return row[2:]
  return False

def db_inode_nlink (c, VERSION, id):
  c.execute ("SELECT nlink FROM inodes WHERE id = ? AND ? >= vmin AND ? <= vmax",
             (id, VERSION, VERSION))
  for row in c:
    return row[0]
  return 0

def ask_user (repo, master, local, c_fmt, m_fmt, l_fmt, filename):
  while True:
    print "======================================================================"
    for i in range (len (c_fmt)):
      print "Common   ", c_fmt[i][0], ":", c_fmt[i][1]
      if c_fmt[i] != m_fmt[i]:
        print " * Master", m_fmt[i][0], ":", m_fmt[i][1]
      if c_fmt[i] != l_fmt[i]:
        print " * Local ", l_fmt[i][0], ":", l_fmt[i][1]
    print "======================================================================"
    print "The following file was modified locally and in the master history:"
    print " => '%s'" % filename
    print "======================================================================"
    print "  l  use local version"
    print "  m  use master version"
    print "  b  keep both versions"
    print "     => master version will be stored as '%s'" % filename
    print "     => local version will be stored as '%s~1'" % filename
    print "  s  start a shell to investigate"
    print "  a  abort pull completely (history will be restored to the original"
    print "     state, before the merge)"
    print "======================================================================"
    line = raw_input ("How should this conflict be resolved? ")
    if line == "l" or line == "m" or line == "b" or line == "a":
      return line
    if line == "s":
      old_cwd = os.getcwd()

      master_file = os.path.join (old_cwd, "objects", make_object_filename (master))
      local_file = os.path.join (old_cwd, "objects", make_object_filename (local))

      default_get = repo.config.get ("default/get")

      if len (default_get) == 0:
        raise Exception ("get: no repository specified and default/get config value empty")
      url = default_get[0]

      remote_repo = RemoteRepo (url)
      get_remote_objects (remote_repo, [ master ])

      os.mkdir ("merge")
      os.chdir ("merge")
      shutil.copyfile (master_file, "master")
      shutil.copyfile (local_file, "local")
      os.system (os.environ['SHELL'])
      try:
        os.remove ("master")
        os.remove ("local")
      except:
        pass
      os.chdir (old_cwd)
      os.rmdir ("merge")
    else:
      print
      print "<<< invalid choice '%s' >>>" % line
      print

def ask_user_del (repo, master_inode, local_inode, filename):
  while True:
    print "======================================================================"
    print "The following file was"
    if master_inode is None:
      print " - deleted in master history"
    else:
      print " - modified in master history"
    if local_inode is None:
      print " - deleted locally"
    else:
      print " - modified locally"
    print " => '%s'" % filename
    print "======================================================================"
    print "  l  use local version"
    print "  m  use master version"
    print "  b  keep both versions"
    print "     => master version will be stored as '%s'" % filename
    print "     => local version will be stored as '%s~1'" % filename
    print "  s  start a shell to investigate"
    print "  a  abort pull completely (history will be restored to the original"
    print "     state, before the merge)"
    print "======================================================================"
    line = raw_input ("How should this conflict be resolved? ")
    if line == "l" or line == "m" or line == "b" or line == "a":
      return line
    else:
      print
      print "<<< invalid choice '%s' >>>" % line
      print



class MergeHistory:
  def __init__ (self):
    self.master_changes = dict()
    self.local_changes = dict()

  def add_change_master (self, version, object, change):
    if not self.master_changes.has_key (object):
      self.master_changes[object]  = []
    self.master_changes[object] += [ (version, change) ]

  def add_change_local (self, version, object, change):
    if not self.local_changes.has_key (object):
      self.local_changes[object]  = []
    self.local_changes[object] += [ (version, change) ]

  def show_one (self, k):
    print "*** Changes for key %s ***" % k
    print "--- Master ---"
    if self.master_changes.has_key (k):
      for entry in self.master_changes[k]:
        print "%4d   : %s" % (entry[0], "|".join (entry[1]))
    print "--- Local ---"
    if self.local_changes.has_key (k):
      for entry in self.local_changes[k]:
        print "%4d   : %s" % (entry[0], "|".join (entry[1]))

  def show (self):
    keys = dict()
    for k in self.master_changes:
      keys[k] = True
    for k in self.local_changes:
      keys[k] = True
    for k in keys:
      self.show_one (k)

  def conflict_keys (self):
    result = []
    keys = dict()
    for k in self.master_changes:
      keys[k] = True
    for k in self.local_changes:
      if keys.has_key (k):
        result += [ k ]
    return result

def apply_inode_changes (inode, changes):
  inode = inode[:]  # copy inode
  for entry in changes:
    version = entry[0]
    change = entry[1]
    if change[0] == "i!":
      assert (change[1] == inode[0])
      for i in range (len (inode)):
        new_field = change[i + 1]
        if new_field != '':
          if isinstance (inode[i], int):
            inode[i] = int (new_field)
          else:
            inode[i] = new_field
    if change[0] == "i-":
      assert (change[1] == inode[0])
      inode = None
  return inode

def pretty_date (sec, nsec):
  return datetime.datetime.fromtimestamp (sec).strftime ("%a, %d %b %Y %H:%M:%S.") + "%09d" % nsec

def pretty_format (inode):
  pp = []
  pp += [ ("id", inode[0]) ]
  pp += [ ("uid", inode[1]) ]
  pp += [ ("gid", inode[2]) ]
  pp += [ ("mode", "%o" % inode[3]) ]
  pp += [ ("type", inode[4]) ]
  pp += [ ("content", inode[5]) ]
  pp += [ ("symlink", inode[6]) ]
  pp += [ ("size", inode[7]) ]
  pp += [ ("major", inode[8]) ]
  pp += [ ("minor", inode[9]) ]
  pp += [ ("nlink", inode[10]) ]
  pp += [ ("ctime", pretty_date (inode[11], inode[12])) ]
  pp += [ ("mtime", pretty_date (inode[13], inode[14])) ]
  return pp

def auto_merge_by_ctime (master_inode, local_inode):
  print "AUTOMERGE"

  m_sec = master_inode[11]
  l_sec = local_inode[11]

  if m_sec > l_sec:
    return "m"
  if l_sec > m_sec:
    return "l"

  # same "sec" setting
  if m_sec > l_sec:
    return "m"
  if l_sec > m_sec:
    return "l"

  m_nsec = master_inode[12]
  l_nsec = local_inode[12]
  if m_nsec > l_nsec:
    return "m"
  if l_nsec > m_nsec:
    return "l"

  # same ctime
  return "m"

def update_nlink_delta (nlink_delta, inode, count):
  if not nlink_delta.has_key (inode):
    nlink_delta[inode] = 0
  nlink_delta[inode] += count

def history_merge (c, repo, local_history, remote_history, pull_args):
  common_version = 0
  for v in range (min (len (local_history), len (remote_history))):
    lh = local_history[v]
    rh = remote_history[v]
    # check version
    assert (lh[0] == v + 1)
    assert (rh[0] == v + 1)
    if lh[1] == rh[1]:
      common_version = v + 1
    else:
      break

  print "merge: last common version:", common_version
  os.system ("bfsync2 revert %d" % common_version)
  print "apply patches:"

  # EXAMINE master/local history for merge conflicts
  merge_h = MergeHistory()

  for rh in remote_history:   # remote history
    if rh[0] > common_version:
      diff = rh[1]
      diff_file = os.path.join ("objects", make_object_filename (diff))

      changes = parse_diff (load_diff (diff))
      for change in changes:
        print "M => ", "|".join (change)
        if change[0] == "i+" or change[0] == "i!" or change[0] == "i-":
          merge_h.add_change_master (rh[0], change[1], change)

      print "applying patch %s" % diff
      os.system ("xzcat %s > tmp-diff" % diff_file)
      f = open ("tmp-diff", "r")
      apply (repo, f, diff)
      f.close()
      os.remove ("tmp-diff")

  for lh in local_history:    # local history
    if lh[0] > common_version:
      diff = lh[1]
      diff_file = os.path.join ("objects", make_object_filename (diff))

      changes = parse_diff (load_diff (diff))
      for change in changes:
        print "local => ", "|".join (change)
        if change[0] == "i+" or change[0] == "i!" or change[0] == "i-":
          merge_h.add_change_local (lh[0], change[1], change)
  inode_ignore_change = dict()
  for ck in merge_h.conflict_keys():
    filename = printable_name (c, ck, common_version)
    print "INODE CONFLICT: %s" % filename
    #merge_h.show_one (ck)
    common_inode = list (db_inode (c, common_version, ck))
    master_inode = apply_inode_changes (common_inode, merge_h.master_changes[ck])
    local_inode  = apply_inode_changes (common_inode, merge_h.local_changes[ck])
    if master_inode is None or local_inode is None:
      if pull_args.always_local:
        choice = "l"
      elif pull_args.always_master:
        choice = "m"
      else:
        choice = ask_user_del (repo, master_inode, local_inode, filename)
    else:
      c_fmt = pretty_format (common_inode)
      m_fmt = pretty_format (master_inode)
      l_fmt = pretty_format (local_inode)
      conf_field_set = set()
      for i in range (len (c_fmt)):
        if c_fmt[i] != m_fmt[i]:
          conf_field_set.add (c_fmt[i][0])
        if c_fmt[i] != l_fmt[i]:
          conf_field_set.add (c_fmt[i][0])
      if conf_field_set.issubset (["nlink", "mtime", "ctime"]):
        choice = auto_merge_by_ctime (master_inode, local_inode)
      else:
        if pull_args.always_local:
          choice = "l"
        elif pull_args.always_master:
          choice = "m"
        else:
          choice = ask_user (repo, master_inode[5], local_inode[5], c_fmt, m_fmt, l_fmt, filename)
      if choice == "l":
        print "... local version will be used"
      elif choice == "m":
        print "... master version will be used"
        inode_ignore_change[ck] = True

  for lh in local_history:
    if lh[0] > common_version:

      # determine current db version
      VERSION = 1
      c.execute ('''SELECT version FROM history''')
      for row in c:
        VERSION = max (row[0], VERSION)

      # adapt diff to get rid of conflicts
      diff = lh[1]
      diff_file = os.path.join ("objects", make_object_filename (diff))

      new_diff_filename = os.path.join (repo.path, "tmp-merge-diff")
      new_diff_file = open (new_diff_filename, "w")

      nlink_delta = dict()
      changes = parse_diff (load_diff (diff))
      for change in changes:
        ignore_change = False
        if change[0] == "l+":
          if db_contains_link (c, VERSION, change[1], change[2]):
            print "LINK CONFLICT"
            suffix = 1
            while db_contains_link (c, VERSION, change[1], change[2] + "~%d" % suffix):
              suffix += 1
            change[2] = change[2] + "~%d" % suffix
        if (change[0] == "i!" or change[0] == "i-" or change[0] == "i+") and inode_ignore_change.has_key (change[1]):
          ignore_change = True
        if not ignore_change:
          # gather information to fix nlink fields
          if change[0] == "l+":
            update_nlink_delta (nlink_delta, change[3], 1)
          if change[0] == "l-":
            old = db_link_inode (c, VERSION, change[1], change[2])
            update_nlink_delta (nlink_delta, old, -1)
          if change[0] == "l!":
            old = db_link_inode (c, VERSION, change[1], change[2])
            update_nlink_delta (nlink_delta, old, -1)
            update_nlink_delta (nlink_delta, change[3], 1)
          # write change to diff
          print " => ", "|".join (change)
          s = ""
          for change_field in change:
            s += change_field + "\0"
          new_diff_file.write (s)

      # fix nlink fields (which may be inaccurate due to master history / local history merge)
      for inode in nlink_delta:
        nlink = (nlink_delta[inode] + db_inode_nlink (c, VERSION, inode))
        if nlink != 0:  # deleted inodes can no longer be modified, so we assume the inode is gone now
          change = [ "i!", inode ] + [ "" ] * 14
          change[11] = "%d" % nlink

          # the resulting diff is no longer normalized (i.e. contains more than one inode change
          # per inode), but bfapply.py will re-normalize it anyway
          s = ""
          for change_field in change:
            s += change_field + "\0"
          new_diff_file.write (s)

      new_diff_file.close()

      # apply modified diff
      print "applying patch %s" % diff

      new_diff_file = open ("tmp-merge-diff")
      changes = parse_diff (new_diff_file.read())
      new_diff_file.close()
      for change in changes:
        print "L => ", "|".join (change)

      new_diff_file = open ("tmp-merge-diff")
      apply (repo, new_diff_file)
      new_diff_file.close()
      #os.system ("bfapply.py < %s" % new_diff_filename)

def cmd_pull():
  parser = argparse.ArgumentParser (prog='bfsync2 pull')
  parser.add_argument ('--always-local', action='store_const', const=True,
                       help='always use local version for merge conflicts')
  parser.add_argument ('--always-master', action='store_const', const=True,
                       help='always use master version for merge conflicts')
  parser.add_argument ('repo', nargs = '?')
  pull_args = parser.parse_args (args)

  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  if pull_args.repo is None:
    default_pull = repo.config.get ("default/pull")
    if len (default_pull) == 0:
      raise Exception ("pull: no repository specified and default/push config value empty")
    url = default_pull[0]
  else:
    url = pull_args.repo

  remote_repo = RemoteRepo (url)
  remote_history = remote_repo.get_history()

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')

  local_history = []
  for row in c:
    local_history += [ row ]

  l_dict = dict()     # dict: version number -> diff hash
  for lh in local_history:
    version = lh[0]
    hash = lh[1]
    if hash:
      l_dict[version] = hash

  ff_apply = []
  transfer_objs = []
  can_fast_forward = True
  for rh in remote_history:
    version = rh[0]
    hash = rh[1]
    if hash:
      transfer_objs += [ hash ]
      if l_dict.has_key (version):
        if hash != l_dict[version]:
          can_fast_forward = False
        else:
          pass    # same version, local and remote
      else:
        ff_apply += [ hash ]

  # transfer required history objects
  get_remote_objects (remote_repo, transfer_objs)

  if can_fast_forward:
    print "will fast-forward %d versions..." % len (ff_apply)
    for diff in ff_apply:
      diff_file = os.path.join ("objects", make_object_filename (diff))
      print "applying patch %s" % diff
      os.system ("xzcat %s > tmp-diff" % diff_file)
      f = open ("tmp-diff", "r")
      apply (repo, f, diff)
      f.close()
      os.remove ("tmp-diff")
  else:
    history_merge (c, repo, local_history, remote_history, pull_args)
  return

def cmd_push():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  if len (args) == 0:
    default_push = repo.config.get ("default/push")
    if len (default_push) == 0:
      raise Exception ("push: no repository specified and default/push config value empty")
    url = default_push[0]
  else:
    url = args[0]

  remote_repo = RemoteRepo (url)
  remote_history = remote_repo.get_history()

  c = conn.cursor()
  c.execute ('''SELECT * FROM history WHERE hash != '' ORDER BY version''')

  local_history = []
  for row in c:
    local_history += [ row ]

  common_version = 0
  for v in range (min (len (local_history), len (remote_history))):
    lh = local_history[v]
    rh = remote_history[v]
    # check version
    assert (lh[0] == v + 1)
    assert (rh[0] == v + 1)
    if lh[1] == rh[1]:
      common_version = v + 1
    else:
      break
  if common_version != len (remote_history):
    raise Exception ("push failed, remote history contains commits not in local history (pull to fix this)")

  delta_history = []
  for v in range (len (local_history)):
    if v + 1 > common_version:
      delta_history += [ local_history[v] ]

  print remote_repo.update_history (delta_history)

  need_objs = remote_repo.need_objects()

  tl = TransferList()
  for hash in need_objs:
    src_file = os.path.join ("objects", make_object_filename (hash))
    if validate_object (src_file, hash):
      tl.add (TransferFile (src_file, os.path.join (remote_repo.path, src_file), os.path.getsize (src_file), 0400))

  remote_repo.put_objects (tl)
  return

def get_remote_objects (remote_repo, transfer_objs):
  # make a list of hashes that we need
  need_hash = dict()
  for thash in transfer_objs:
    dest_file = os.path.join ("objects", make_object_filename (thash))
    if not validate_object (dest_file, thash):
      need_hash[thash] = True

  # check for objects in remote repo
  remote_list = remote_repo.ls()
  tlist = TransferList()
  for rfile in remote_list:
    if need_hash.has_key (rfile.hash):
      src_file = os.path.join (remote_repo.path, "objects", make_object_filename (rfile.hash))
      dest_file = os.path.join ("objects", make_object_filename (rfile.hash))
      tlist.add (TransferFile (src_file, dest_file, rfile.size, 0400))

  # do the actual copying
  remote_repo.get_objects (tlist)

def cmd_get():
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path

  if len (args) == 0:
    default_get = repo.config.get ("default/get")
    if len (default_get) == 0:
      raise Exception ("get: no repository specified and default/get config value empty")
    url = default_get[0]
  else:
    url = args[0]

  remote_repo = RemoteRepo (url)

  c = conn.cursor()

  # create list of required objects
  objs = []
  c.execute ('''SELECT DISTINCT hash FROM inodes''')
  for row in c:
    s = "%s" % row[0]
    if len (s) == 40:
      objs += [ s ]

  get_remote_objects (remote_repo, objs)

def cmd_init():
  dir = args[0]
  try:
    os.mkdir (dir, 0700)
  except:
    raise Exception ("can't create directory %s for repository" % dir)
  bfsync_dir = os.path.join (dir, ".bfsync")
  os.mkdir (bfsync_dir)
  f = open (os.path.join (bfsync_dir, "info"), "w")
  f.write ("repo-type master;")
  f.close()
  f = open (os.path.join (bfsync_dir, "config"), "w")
  f.write ("sqlite-sync 1;")
  f.close()

  os.chdir (dir)

  # create objects directory
  os.mkdir ("objects", 0700)
  for i in range (0, 256):
    os.mkdir ("objects/%02x" % i, 0700)

  # create history table
  repo = cd_repo_connect_db()
  conn = repo.conn
  c = conn.cursor()
  c.execute ('''CREATE TABLE history
                 (
                   version integer,
                   hash    text,
                   author  text,
                   message text,
                   time    integer
                 )''')
  conn.commit()

  # create initial commit diff
  time_now = int (time.time())
  change_list = [
    "i+",
    "0" * 40,           # id (root inode)
    "%d" % os.getuid(), # uid
    "%d" % os.getgid(), # gid
    "%d" % 0755,        # mode
    "dir",              # type
    "", "", "0", "0", "0", "1",
    "%d" % time_now, "0", # ctime
    "%d" % time_now, "0"  # mtime
  ]

  f = open ("init-diff", "w")
  for s in change_list:
    f.write (s + "\0")
  f.close()
  os.system ("xz -9 init-diff")
  hash = move_file_to_objects (repo, "init-diff.xz")

  # create initial history entry
  c.execute ("""INSERT INTO history VALUES (1, ?, "no author", "initial commit", ?)""", (hash, time_now))
  conn.commit()

def guess_dir_name (url):
  dir_name = ""
  for ch in reversed (url):
    if (ch == ":") or (ch == "/"):
      return dir_name
    dir_name = ch + dir_name
  return url

def cmd_clone():
  url = args[0]
  if len (args) > 1:
    dir = args[1]
  else:
    dir = guess_dir_name (args[0])
  if os.path.exists (dir):
    print "fatal: destination path '" + dir + "' already exists"
    sys.exit (1)

  url_list = url.split (":")
  if len (url_list) == 1:
    # local repository => use absolute path
    url = os.path.abspath (url)

  print url, "=>", dir

  try:
    os.mkdir (dir, 0700)
  except:
    raise Exception ("can't create directory %s for repository" % dir)

  bfsync_dir = os.path.join (dir, ".bfsync")
  os.mkdir (bfsync_dir)

  # init .bfsync/info
  f = open (os.path.join (bfsync_dir, "info"), "w")
  f.write ("repo-type store;")
  f.close()

  # default config
  f = open (os.path.join (bfsync_dir, "config"), "w")
  f.write ("sqlite-sync 1;\n")
  f.write ("default {\n")
  f.write ("""  pull "%s";\n""" % url)
  f.write ("""  push "%s";\n""" % url)
  f.write ("}\n")
  f.close()

  os.chdir (dir)
  repo = cd_repo_connect_db()
  conn = repo.conn
  repo_path = repo.path
  c = conn.cursor()
  create_tables (c)
  init_tables (c)
  conn.commit()

  os.mkdir ("new", 0700)
  os.mkdir ("objects", 0700)
  for i in range (0, 256):
    os.mkdir ("new/%02x" % i, 0700)
    os.mkdir ("objects/%02x" % i, 0700)

  # mount for pull
  os.mkdir ("clone-mnt", 0700)
  if subprocess.call (["bfsyncfs", ".", "clone-mnt"]) != 0:
    raise Exception ("cannot mount repo")
  os.chdir ("clone-mnt")
  if subprocess.call (["bfsync2", "pull", url]) != 0:
    raise Exception ("cannot pull from repo")
  os.chdir ("..")
  if subprocess.call (["fusermount", "-u", "clone-mnt"]) != 0:
    raise Exception ("cannot umount repo")
  os.rmdir ("clone-mnt")
  # FIXME: ensure repo deletion on exception

command = None
command_func = None
arg_iter = sys.argv[1:].__iter__()
args = []

for arg in arg_iter:
  commands = [
    ( "commit",                 cmd_commit, 1),
    ( "log",                    cmd_log, 0),
    ( "pull",                   cmd_pull, 1),
    ( "push",                   cmd_push, 1),
    ( "get",                    cmd_get, 1),
    ( "status",                 cmd_status, 0),
    ( "revert",                 cmd_revert, 1),
    ( "init",                   cmd_init, 1),
    ( "clone",                  cmd_clone, 1),
    ( "db-fingerprint",         cmd_db_fingerprint, 0),
    ( "remote",                 cmd_remote, 1),
    ( "debug-load-all-inodes",  cmd_debug_load_all_inodes, 0),
    ( "debug-perf-getattr",     cmd_debug_perf_getattr, 1),
    ( "debug-clear-cache",      cmd_debug_clear_cache, 1),
    ( "debug-integrity",        cmd_debug_integrity, 0),
  ]
  parse_ok = False
  if command == None:
    for c in commands:
      if c[0] == arg:
        command_func = c[1]
        command_args = c[2]
        command = c[0]
        parse_ok = True
  else:
    if command_args > 0:
      args += [ arg ]
      parse_ok = True
  if not parse_ok:
    sys.stderr.write ("can't parse command line args...\n")
    sys.exit (1)

if command_func != None:
  try:
    if False: # profiling
      import cProfile

      cProfile.run ("command_func()", "/tmp/bfsync2-profile-%s" % command)
    else:
      command_func()
  except Exception, ex:
    print "\n\n"
    print "=================================================="
    traceback.print_exc()
    print "=================================================="
    print "\n\n"
    hash_cache.save()
    sys.stderr.write ("bfsync2: %s\n" % ex)
    sys.exit (1)
  hash_cache.save()
else:
  print "usage: bfsync <command> [ args... ]"
